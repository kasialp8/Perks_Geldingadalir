{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Data Download Notebook\n",
    "\n",
    "Zali et al. 2024 pulls horizontal seismic data from a single horizontal component from a single station, demeaned and detrended but not converted from instrument response. The data is pulled from March 12 to June 24, ~104 days, beginning 7 days before the Geldingadalir eruption began. The data is also downsampled to the minimum sampling rate necessary to observe the local volcanic tremor.\n",
    "\n",
    "*Important Note:* This notebook runs using a different environment than this project's computational notebook. ObsPy is required, along with numpy and matplotlib\n",
    "\n",
    "This notebook will download 100 days of data, beginning on March 1st, 2005 and ending June 9th, 2005, from sensor CC.STD of the Cascade Chain Volcano Monitoring network, located on Mount St. Helens on Studebaker Ridge. Mount St. Helens had a series of eruptions of ash and steam between 2004 and 2008, but this will focus on the second explosive eruption on March 8th, 2005 that was preceeded by seismic activity. https://www.usgs.gov/volcanoes/mount-st.-helens/science/2004-2008-renewed-volcanic-activity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting obspy\n",
      "  Using cached obspy-1.4.1-cp311-cp311-macosx_10_13_x86_64.whl\n",
      "Collecting numpy>=1.20 (from obspy)\n",
      "  Downloading numpy-2.1.3-cp311-cp311-macosx_14_0_x86_64.whl.metadata (62 kB)\n",
      "Collecting scipy>=1.7 (from obspy)\n",
      "  Using cached scipy-1.14.1-cp311-cp311-macosx_14_0_x86_64.whl.metadata (60 kB)\n",
      "Collecting matplotlib>=3.3 (from obspy)\n",
      "  Using cached matplotlib-3.9.2-cp311-cp311-macosx_10_12_x86_64.whl.metadata (11 kB)\n",
      "Collecting lxml (from obspy)\n",
      "  Using cached lxml-5.3.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: setuptools in /Users/KatarzynaPerks/Documents/GitHub/Perks_Geldingadalir/.conda/lib/python3.11/site-packages (from obspy) (75.3.0)\n",
      "Collecting sqlalchemy<2 (from obspy)\n",
      "  Using cached SQLAlchemy-1.4.54-cp311-cp311-macosx_10_9_universal2.whl.metadata (10 kB)\n",
      "Requirement already satisfied: decorator in /Users/KatarzynaPerks/Documents/GitHub/Perks_Geldingadalir/.conda/lib/python3.11/site-packages (from obspy) (5.1.1)\n",
      "Collecting requests (from obspy)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib>=3.3->obspy)\n",
      "  Using cached contourpy-1.3.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib>=3.3->obspy)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib>=3.3->obspy)\n",
      "  Using cached fonttools-4.54.1-cp311-cp311-macosx_10_9_universal2.whl.metadata (163 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib>=3.3->obspy)\n",
      "  Using cached kiwisolver-1.4.7-cp311-cp311-macosx_10_9_x86_64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/KatarzynaPerks/Documents/GitHub/Perks_Geldingadalir/.conda/lib/python3.11/site-packages (from matplotlib>=3.3->obspy) (24.1)\n",
      "Collecting pillow>=8 (from matplotlib>=3.3->obspy)\n",
      "  Using cached pillow-11.0.0-cp311-cp311-macosx_10_10_x86_64.whl.metadata (9.1 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib>=3.3->obspy)\n",
      "  Using cached pyparsing-3.2.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/KatarzynaPerks/Documents/GitHub/Perks_Geldingadalir/.conda/lib/python3.11/site-packages (from matplotlib>=3.3->obspy) (2.9.0)\n",
      "Collecting greenlet!=0.4.17 (from sqlalchemy<2->obspy)\n",
      "  Using cached greenlet-3.1.1-cp311-cp311-macosx_11_0_universal2.whl.metadata (3.8 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->obspy)\n",
      "  Using cached charset_normalizer-3.4.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->obspy)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->obspy)\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->obspy)\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/KatarzynaPerks/Documents/GitHub/Perks_Geldingadalir/.conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib>=3.3->obspy) (1.16.0)\n",
      "Using cached matplotlib-3.9.2-cp311-cp311-macosx_10_12_x86_64.whl (7.9 MB)\n",
      "Downloading numpy-2.1.3-cp311-cp311-macosx_14_0_x86_64.whl (6.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached scipy-1.14.1-cp311-cp311-macosx_14_0_x86_64.whl (25.5 MB)\n",
      "Using cached SQLAlchemy-1.4.54-cp311-cp311-macosx_10_9_universal2.whl (1.6 MB)\n",
      "Using cached lxml-5.3.0-cp311-cp311-macosx_10_9_x86_64.whl (4.4 MB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Using cached charset_normalizer-3.4.0-cp311-cp311-macosx_10_9_x86_64.whl (124 kB)\n",
      "Using cached contourpy-1.3.0-cp311-cp311-macosx_10_9_x86_64.whl (266 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.54.1-cp311-cp311-macosx_10_9_universal2.whl (2.8 MB)\n",
      "Using cached greenlet-3.1.1-cp311-cp311-macosx_11_0_universal2.whl (272 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached kiwisolver-1.4.7-cp311-cp311-macosx_10_9_x86_64.whl (65 kB)\n",
      "Using cached pillow-11.0.0-cp311-cp311-macosx_10_10_x86_64.whl (3.2 MB)\n",
      "Using cached pyparsing-3.2.0-py3-none-any.whl (106 kB)\n",
      "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Installing collected packages: urllib3, pyparsing, pillow, numpy, lxml, kiwisolver, idna, greenlet, fonttools, cycler, charset-normalizer, certifi, sqlalchemy, scipy, requests, contourpy, matplotlib, obspy\n",
      "Successfully installed certifi-2024.8.30 charset-normalizer-3.4.0 contourpy-1.3.0 cycler-0.12.1 fonttools-4.54.1 greenlet-3.1.1 idna-3.10 kiwisolver-1.4.7 lxml-5.3.0 matplotlib-3.9.2 numpy-2.1.3 obspy-1.4.1 pillow-11.0.0 pyparsing-3.2.0 requests-2.32.3 scipy-1.14.1 sqlalchemy-1.4.54 urllib3-2.2.3\n"
     ]
    }
   ],
   "source": [
    "!pip install obspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import obspy\n",
    "from obspy import UTCDateTime as utc\n",
    "from obspy.clients.fdsn import Client\n",
    "\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Description\n",
    "\n",
    "The seismic data is downloaded from the IRIS/Earthscope Database using ObsPy. The data downloads as a trace, an array of data with attached metadata, which is then packaged into a stream, which can contain multiple traces. The daily seismic data records are then saved as [mseed files](https://ds.iris.edu/ds/nodes/dmc/data/formats/miniseed/), which preserve this data+metadata structure, but requires ObsPy or other specialized software to open."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set all of your variables for the station:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client('IRIS')\n",
    "\n",
    "#creating variables to download data\n",
    "starttime = utc('2005-03-01T00:00:00')\n",
    "endtime = starttime + 100 * (60*60*24)\n",
    "day_range = 100\n",
    "\n",
    "#also add a buffer to both ends to chop off once the data has been filtered\n",
    "#and downsampled, kind of arbitrary length, 5% of a day (default ObsPy taper length)\n",
    "buffer = 60*60*24*0.05 #seconds\n",
    "\n",
    "net = 'CC'\n",
    "sta = 'STD'\n",
    "loc = '*' #wildcard, generally don't care about location code\n",
    "cha = 'BHN' #horizontal component, as used in Zali et al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from 2005-03-01T00:00:00.000000Z to 2005-06-09T00:00:00.000000Z\n",
      "Downloaded day 1\n",
      "Downloaded day 2\n",
      "Downloaded day 3\n",
      "Downloaded day 4\n",
      "Downloaded day 5\n",
      "Downloaded day 6\n",
      "Downloaded day 7\n",
      "Downloaded day 8\n",
      "Downloaded day 9\n",
      "Downloaded day 10\n",
      "Downloaded day 11\n",
      "Downloaded day 12\n",
      "Downloaded day 13\n",
      "Downloaded day 14\n",
      "Downloaded day 15\n",
      "Downloaded day 16\n",
      "Downloaded day 17\n",
      "Downloaded day 18\n",
      "Downloaded day 19\n",
      "Downloaded day 20\n",
      "Downloaded day 21\n",
      "Downloaded day 22\n",
      "Downloaded day 23\n",
      "Downloaded day 24\n",
      "Downloaded day 25\n",
      "Downloaded day 26\n",
      "Downloaded day 27\n",
      "Downloaded day 28\n",
      "Error downloading data for day 29: No data available for request.\n",
      "HTTP Status code: 204\n",
      "Detailed response of server:\n",
      "\n",
      "\n",
      "Error downloading data for day 30: No data available for request.\n",
      "HTTP Status code: 204\n",
      "Detailed response of server:\n",
      "\n",
      "\n",
      "Error downloading data for day 31: No data available for request.\n",
      "HTTP Status code: 204\n",
      "Detailed response of server:\n",
      "\n",
      "\n",
      "Error downloading data for day 32: No data available for request.\n",
      "HTTP Status code: 204\n",
      "Detailed response of server:\n",
      "\n",
      "\n",
      "Error downloading data for day 33: No data available for request.\n",
      "HTTP Status code: 204\n",
      "Detailed response of server:\n",
      "\n",
      "\n",
      "Error downloading data for day 34: No data available for request.\n",
      "HTTP Status code: 204\n",
      "Detailed response of server:\n",
      "\n",
      "\n",
      "Downloaded day 35\n",
      "Downloaded day 36\n",
      "Downloaded day 37\n",
      "Downloaded day 38\n",
      "Downloaded day 39\n",
      "Downloaded day 40\n",
      "Downloaded day 41\n",
      "Downloaded day 42\n",
      "Downloaded day 43\n",
      "Downloaded day 44\n",
      "Downloaded day 45\n",
      "Downloaded day 46\n",
      "Downloaded day 47\n",
      "Downloaded day 48\n",
      "Downloaded day 49\n",
      "Downloaded day 50\n",
      "Downloaded day 51\n",
      "Downloaded day 52\n",
      "Downloaded day 53\n",
      "Downloaded day 54\n",
      "Downloaded day 55\n",
      "Downloaded day 56\n",
      "Downloaded day 57\n",
      "Downloaded day 58\n",
      "Downloaded day 59\n",
      "Downloaded day 60\n",
      "Downloaded day 61\n",
      "Downloaded day 62\n",
      "Downloaded day 63\n",
      "Downloaded day 64\n",
      "Downloaded day 65\n",
      "Downloaded day 66\n",
      "Downloaded day 67\n",
      "Downloaded day 68\n",
      "Downloaded day 69\n",
      "Downloaded day 70\n",
      "Downloaded day 71\n",
      "Downloaded day 72\n",
      "Downloaded day 73\n",
      "Downloaded day 74\n",
      "Downloaded day 75\n",
      "Downloaded day 76\n",
      "Downloaded day 77\n",
      "Downloaded day 78\n",
      "Downloaded day 79\n",
      "Downloaded day 80\n",
      "Downloaded day 81\n",
      "Downloaded day 82\n",
      "Downloaded day 83\n",
      "Downloaded day 84\n",
      "Downloaded day 85\n",
      "Downloaded day 86\n",
      "Downloaded day 87\n",
      "Downloaded day 88\n",
      "Downloaded day 89\n",
      "Downloaded day 90\n",
      "Downloaded day 91\n",
      "Downloaded day 92\n",
      "Downloaded day 93\n",
      "Downloaded day 94\n",
      "Downloaded day 95\n",
      "Downloaded day 96\n",
      "Downloaded day 97\n",
      "Downloaded day 98\n",
      "Downloaded day 99\n",
      "Downloaded day 100\n",
      "data download complete, saved to /Users/KatarzynaPerks/Documents/GitHub/Perks_Geldingadalir/notebooks/HVUWB_analysis/data/raw/\n"
     ]
    }
   ],
   "source": [
    "#create folder for numpy streams to go into and initialize filepath\n",
    "!mkdir data\n",
    "!mkdir data/raw\n",
    "filepath = os.getcwd() + '/data/raw/'\n",
    "\n",
    "#create arrays to save dates\n",
    "dates = np.array([])\n",
    "\n",
    "print(f'Downloading data from {starttime} to {endtime}')\n",
    "\n",
    "#download the data piecemeal, here by day\n",
    "for day in range(day_range):\n",
    "    \n",
    "    tr_length = 24*60*60\n",
    "    \n",
    "    # Format the current date as YYYYMMDD for the filename\n",
    "    date_str = starttime.strftime('%Y%m%d')\n",
    "    filename = f\"{day + 1}_{date_str}_{net}{sta}.mseed\"\n",
    "    \n",
    "    # if file already exists, then skip\n",
    "    if os.path.exists(filepath + filename,):\n",
    "        print(f'Data for day {date_str} already exists, skipping')\n",
    "        starttime += tr_length\n",
    "    else:\n",
    "        try:\n",
    "            # actually downloading\n",
    "            st = client.get_waveforms(network=net,\n",
    "                                    station=sta,\n",
    "                                    location=loc,\n",
    "                                    channel=cha,\n",
    "                                    starttime=starttime-buffer,\n",
    "                                    endtime=starttime+buffer+tr_length)\n",
    "\n",
    "            # instrument sampling rate (hz)\n",
    "            freq = st[0].stats.sampling_rate\n",
    "\n",
    "            # merge traces within stream, linearly interpolating any gaps\n",
    "            st.merge(fill_value='interpolate')\n",
    "\n",
    "            ## Save data as MSEED, standard for storing seismic data\n",
    "            st.write(filepath + filename, format='MSEED')\n",
    "\n",
    "            # adding date\n",
    "            dates = np.append(dates, starttime.date)\n",
    "\n",
    "            print(f'Downloaded day {day + 1}')\n",
    "        except Exception as e:\n",
    "            print(f'Error downloading data for day {day + 1}: {e}')\n",
    "        \n",
    "    starttime += tr_length\n",
    "\n",
    "#save dates list for future use\n",
    "np.save(filepath+'date_list.csv', dates)\n",
    "\n",
    "print(f'data download complete, saved to {filepath}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "### Data Modalities and Formats\n",
    "\n",
    "#### Data Modalities\n",
    "The dataset consists of seismic data collected from a single horizontal component (BHN) of the CC.STD. The data is recorded continuously over a period of 100 days, capturing the seismic activity before, during, and after the volcanic eruption.\n",
    "\n",
    "#### Data Formats\n",
    "1. **MSEED (Mini-SEED) Files**:\n",
    "    - The seismic data is stored in Mini-SEED format, which is a compact binary format used for storing time series data. Each file contains a day's worth of seismic data, including metadata such as the sampling rate and station information.\n",
    "    - Example file name: `1_YYYYMMDD_hvuwb.mseed`, `2_YYYYMMDD_hwuwb.mseed`, ..., `60_YYYYMMDD_hvuwb.mseed`.\n",
    "2. **Numpy Arrays**:\n",
    "    - The dates corresponding to each day's seismic data are stored in a numpy array and saved as a CSV file (`date_list.csv`). This array helps in mapping the MSEED files to their respective dates.\n",
    "    - Example: `dates = np.array([datetime.date(2021, 5, 16), datetime.date(2021, 5, 17), ...])`.\n",
    "\n",
    "#### Data Processing\n",
    "- The raw seismic data is downloaded using the ObsPy library from the IRIS/Earthscope Database.\n",
    "- The data is merged and interpolated to fill any gaps, ensuring a continuous time series.\n",
    "- The processed data is saved in the MSEED format, preserving both the time series and metadata.\n",
    "\n",
    "This structured approach ensures that the seismic data is well-organized and easily accessible for further analysis and processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Trace(s) in Stream:\n",
      "CC.STD..BHN | 2005-04-09T22:48:00.000000Z - 2005-04-11T01:12:00.000000Z | 50.0 Hz, 4752001 samples\n"
     ]
    }
   ],
   "source": [
    "from obspy import read\n",
    "import glob\n",
    "\n",
    "# Folder containing mseed files\n",
    "#folder_path = '/path/to/your/folder'\n",
    "\n",
    "filepath = os.getcwd() + '/data/raw/'\n",
    "\n",
    "# Get the first mseed file in the folder\n",
    "first_file = glob.glob(f\"{filepath}/*.mseed\")[0]\n",
    "\n",
    "# Read and display the file\n",
    "st = read(first_file)\n",
    "print(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
